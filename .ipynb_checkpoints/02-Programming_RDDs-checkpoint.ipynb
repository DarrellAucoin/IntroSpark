{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming with RDDs\n",
    "\n",
    "Spark automatically distributes the data in the RDD across a cluster and performs operations on them.\n",
    "\n",
    "## RDD Basics\n",
    "\n",
    "### Creating a RDD from Files\n",
    "\n",
    "\n",
    "#### Creating RDD from Textfiles (i.e. CSV files)\n",
    "We can create a RDD from a text file where each new line is a new record. We can then transform this RDD into the dataset that we want with RDD operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'# Apache Spark',\n",
       " u'',\n",
       " u'Spark is a fast and general cluster computing system for Big Data. It provides',\n",
       " u'high-level APIs in Scala, Java, and Python, and an optimized engine that',\n",
       " u'supports general computation graphs for data analysis. It also supports a']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"file:/usr/local/spark/1.4.0/README.md\")\n",
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating RDD from JSON file\n",
    "\n",
    "There is a special kind of RDD called a DataFrame (much like the dataframes in R or Pandas in Python). You can read json files into this kind of RDD.\n",
    "\n",
    "More on DataFrames and Spark-SQL later.\n",
    "\n",
    "```Python\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = sqlContext.read.json(\"jsonFilePath\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating RDD from Parquet file\n",
    "\n",
    "The Parquet file format is a columnar storage developed specifically for the Hadoop ecosystem and is the default file format for DataFrames in Spark.\n",
    "\n",
    "```Python\n",
    "SQLContext.read.parquet(\"parquetFilePath\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generic RDD (DataFrame) Creation\n",
    "\n",
    "DataFrames can be loaded and saved into a variety of formats. The default of which is [Parquet file format](https://parquet.apache.org/).\n",
    "\n",
    "```Python\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.load(\"filePathToParquetFile\")\n",
    "```\n",
    "\n",
    "`sqlContext.read.load` can read files from json, parquet, or jdbc:\n",
    "\n",
    "```Python\n",
    "df = sqlContext.read.load(\"filePathToJsonFile\", format=\"json\")\n",
    "```\n",
    "\n",
    "```Python\n",
    "df = sqlContext.read.load(\"filePathToParquetFile\", format=\"parquet\")\n",
    "```\n",
    "\n",
    "```Python\n",
    "df = sqlContext.read.load(\"filePathToParquetFile\", format=\"jdbc\")\n",
    "```\n",
    "\n",
    "Dataframes can also be saved by specifying the format, with the default as `parquet`.\n",
    "\n",
    "\n",
    "```Python\n",
    "df.write.save(\"FileName.parquet\")\n",
    "```\n",
    "\n",
    "```Python\n",
    "df.write.save(\"FileName.parquet\", format=\"parquet\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Two Types of RDD Operations: Transformations and Actions\n",
    "\n",
    "There are two types of RDD operations: transformations and actions.\n",
    "\n",
    "__Transformations__ construct a new RDD from the previous one. (mapping, filtering, etc.)\n",
    "\n",
    "__Actions__ Compute a result based on an RDD and either return it to the driver program or save it to a file.\n",
    "\n",
    "Spark performs these operations in a lazy fashion. Spark constructs a Directed Acyclic Graph (DAG) based on the transformations given and only groups them into tasks when an action is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
