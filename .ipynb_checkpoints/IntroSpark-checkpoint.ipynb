{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "<style>\n",
    "\n",
    "@font-face {\n",
    "    font-family: \"Computer Modern\";\n",
    "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
    "}\n",
    "#notebook_panel { /* main background */\n",
    "    background: #888;\n",
    "    color: #f6f6f6;\n",
    "}\n",
    "#notebook li { /* More space between bullet points */\n",
    "margin-top:0.8em;\n",
    "}\n",
    "div.text_cell_render{\n",
    "    font-family: 'Arvo' sans-serif;\n",
    "    line-height: 110%;\n",
    "    font-size: 135%;\n",
    "    width:1000px;\n",
    "    margin-left:auto;\n",
    "    margin-right:auto;\n",
    "}\n",
    "\n",
    "</style>\n",
    "\n",
    "\n",
    "<center>\n",
    "\n",
    "<p class=\"gap05\"<p>\n",
    "<h1>Intro to</h1>\n",
    "</center>\n",
    "<center>\n",
    "<img src=\"images/spark-logo.png\" alt=\"Spark Logo\" >\n",
    "\n",
    "\n",
    "</center>\n",
    "<p class=\"gap05\"<p>\n",
    "\n",
    "<p class=\"gap05\"<p>\n",
    "<center>\n",
    "<h3>Darrell Aucoin</h3>\n",
    "</center>\n",
    "<center>\n",
    "\n",
    "<h3>Stats Club</h3>\n",
    "\n",
    "</center>\n",
    "\n",
    "<p class=\"gap2\"<p>\n",
    "\n",
    "</center>\n",
    "<style type=\"text/css\">\n",
    ".input_prompt, .input_area, .output_prompt \n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "There are several computational environments:  \n",
    "- __Shared CPU, RAM, and Harddrive__: Single core computers\n",
    "- __Shared RAM and Harddrive__: Modern multi-core computers\n",
    "- __Shared Harddrive__: Some server clusters (sharcnet)\n",
    "- __Nothing Shared__: Hadoop Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark Overview\n",
    "Apache Spark is a alternative to the MapReduce in the Hadoop environment. \n",
    "\n",
    "Spark is more flexiable and faster than YARN (Hadoop MapReduce) for iterative algorithms (many Machine Learning algorithms).\n",
    "\n",
    "Modules built on Spark:  \n",
    "- __Spark Streaming__: processing real-time data streams\n",
    "- __Spark SQL and DataFrames__: support for structured data and relational queries\n",
    "- __MLlib__: built-in machine learning library\n",
    "- __GraphX__: Spark’s new API for graph processing\n",
    "- __Bagel (Pregel on Spark)__: older, simple graph processing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are several API's for using Spark. Listed in order of mature development:\n",
    "- [Spark Scala API (Scaladoc)](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package)\n",
    "- [Spark Java API (Javadoc)](https://spark.apache.org/docs/latest/api/java/index.html)\n",
    "- [Spark Python API (Sphinx)](https://spark.apache.org/docs/latest/api/python/index.html)\n",
    "- [Spark R API (Roxygen2)](https://spark.apache.org/docs/latest/api/R/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Installation of Spark in Local Mode (Mac & Linux)\n",
    "1. Download the [Scala](http://www.scala-lang.org/download/) and [Spark](https://spark.apache.org/downloads.html), version 1.4.0 as of this writing. Spark is programmed in Scala. Use version 2.10.x Scala if using Java 7 and don't want to compile Spark from source. Scala 2.11.x uses Java 8 and Spark will need to be compiled from source. The version of Java that your a using can be found via terminal:\n",
    "```Bash\n",
    "$ java -version\n",
    "```\n",
    "\n",
    "2. Make the destination directory (/usr/local/scala/2.10.5 and /usr/local/spark/1.4.0 here) and change ownership to yourself\n",
    "```Bash\n",
    "$ sudo mkdir -p /usr/local/scala/2.10.5\n",
    "$ sudo mkdir -p /usr/local/spark/1.4.0\n",
    "$ sudo chown -R $USER /usr/local/scala\n",
    "$ sudo chown -R $USER /usr/local/spark\n",
    "```\n",
    "\n",
    "3. Uncompress the scala and spark tar file into your new destination\n",
    "```Bash\n",
    "$ cd ~/Downloads\n",
    "$ tar -xzvf scala-2.10.5.tgz /usr/local/scala/2.10.5\n",
    "$ tar -xzvf spark-1.4.0-bin-hadoop2.6.tgz /usr/local/spark/1.4.0\n",
    "```\n",
    "\n",
    "4. For Linux locate your Java home directory (called java_home_dir in step 6). This can be found by the terminal command:\n",
    "```Bash\n",
    "$ sudo update-alternatives --config java\n",
    "```\n",
    "\n",
    "5. For Mac edit `~/.bash_profile` to contain i.e. `vim ~/.bash_profile`:  \n",
    "```Bash\n",
    "# JAVA Home\n",
    "export JAVA_HOME=`/usr/libexec/java_home`\n",
    "# Scala 2.10.5\n",
    "export SCALA_HOME=/usr/local/scala/2.10.5\n",
    "export PATH=\"${SCALA_HOME}/bin:$PATH\"\n",
    "# Spark 1.4.0\n",
    "export SPARK_HOME=/usr/local/scala/2.10.5\n",
    "export PATH=\"${SPARK_HOME}/bin:${SPARK_HOME}/sbin:$PATH\"\n",
    "```\n",
    "\n",
    "6. For Linux edit `~/.bashrc` to contain i.e. `vim ~/.bash_rc`:  \n",
    "```Bash\n",
    "# JAVA Home\n",
    "export JAVA_HOME=java_home_dir\n",
    "# Scala 2.10.5\n",
    "export SCALA_HOME=/usr/local/scala/2.10.5\n",
    "export PATH=\"${SCALA_HOME}/bin:$PATH\"\n",
    "# Spark 1.4.0\n",
    "export SPARK_HOME=/usr/local/scala/2.10.5\n",
    "export PATH=\"${SPARK_HOME}/bin:${SPARK_HOME}/sbin:$PATH\"\n",
    "```\n",
    "\n",
    "6. Ensure bash knows these variabes:\n",
    "```Bash\n",
    "$ source ~/.bash_profile\n",
    "or for linux\n",
    "$ source ~/.bashrc\n",
    "```\n",
    "8. (Optional) Configure ipython and ipython notebook for Spark. In `~/.bash_profile` (Mac) or `~/.bashrc` (Linux) add lines:\n",
    "```Bash\n",
    "# Spark for ipython\n",
    "alias ipyspark=\"IPYTHON=1 pyspark\"\n",
    "alias ipyspark_notebook=\"IPYTHON_OPTS=notebook pyspark\"\n",
    "export PATH=\"${SPARK_HOME}/python/:${PATH}\"\n",
    "export PYSPARK_SUBMIT_ARGS=\"--master local[*]\"\n",
    "```\n",
    "8. (Optional) Install sbt (Scala Build Tool) for using Spark with Scala interface. See [installing sbt](http://www.scala-sbt.org/0.13/tutorial/Setup.html)\n",
    "9. Test if Spark is working. Execute the command:\n",
    "```Bash\n",
    "$ run-example SparkPi 10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MapReduce Overview\n",
    "\n",
    "__Definition.__ _MapReduce_ is a programming paradigm model of using parallel, distributed algorithims to process or generate data sets. MapRedeuce is composed of two main functions:\n",
    "\n",
    "__Map(k,v)__: Filters and sorts data.\n",
    "\n",
    "__Reduce(k,v)__: Aggregates data according to keys (k).\n",
    "\n",
    "MapReduce is broken down into several steps:\n",
    "\n",
    "1. Record Reader\n",
    "2. Map\n",
    "3. Combiner (Optional)\n",
    "4. Partitioner\n",
    "5. Shuffle and Sort\n",
    "6. Reduce\n",
    "7. Output Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![alt text](images/MapReduce.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MapReduce Phases\n",
    "\n",
    "1. __Record Reader__ Translates an input into records to be processed by the user-defined map function in the form of a key-value pair on each map cluster. \n",
    "\n",
    "2. __Map__ _User defined function_ outputing intermediate key-value pairs for the reducers\n",
    "\n",
    "    - __key__ ($k_{2}$): Later, MapReduce will group and possibly aggregate data according to these keys, choosing the right keys is here is important for a good MapReduce job.\n",
    "\n",
    "    - __value__ ($v_{2}$): The data to be grouped according to it's keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. __Record Reader__\n",
    "2. __Map__\n",
    "3. __Combiner__ _User defined function_ that aggregates data according to intermediate keys on a mapper node\n",
    "    - This can usually reduce the amount of data to be sent over the network increasing efficiency\n",
    "    $$\\left.\\begin{array}{r}\n",
    "\\left(\\mbox{\"hello world\"},1\\right)\\\\\n",
    "\\left(\\mbox{\"hello world\"},1\\right)\\\\\n",
    "\\left(\\mbox{\"hello world\"},1\\right)\n",
    "\\end{array}\\right\\} \\overset{\\mbox{combiner}}{\\longrightarrow}\\left(\\mbox{\"hello world\"},3\\right) $$\n",
    "\n",
    "4. __Partitioner__ Sends intermediate key-value pairs (k,v) to reducer by\n",
    "    $$\\mbox{Reducer}=\\mbox{hash}\\left(\\mbox{k}\\right)\\pmod{R}$$  \n",
    "5. __Shuffle and Sort__ On reducer node, sorts by key to help group equivalent keys\n",
    "6. __Reduce__ _User Defined Function_ that aggregates data (v) according to keys (k) to send key-value pairs to output\n",
    "7. __Output Format__ Translates final key-value pairs to file format (tab-seperated by default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![alt text](images/MapReduce.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MapReduce Example: Word Count\n",
    "\n",
    "![alt text](http://xiaochongzhang.me/blog/wp-content/uploads/2013/05/MapReduce_Work_Structure.png)  \n",
    "Image Source: [Xiaochong Zhang's Blog](http://xiaochongzhang.me/blog/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark's DAG Model\n",
    "A more flexible form of MapReduce is used by Spark using __Directed Acyclic Graphs (DAG)__. \n",
    "![alt text](images/DAG.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For a set of operations:  \n",
    "1. Create a DAG for operations\n",
    "2. Divide DAG into tasks\n",
    "3. Assign tasks to nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Basic Concepts\n",
    "\n",
    "__RDD (Resilient Distributed Dataset)__: RDDs are a  fault-tolerant distributed collection of items distributed across many compute nodes and can be manipulated in parallel. \n",
    "\n",
    "- If a portion of the RDD fails then only a portion of RDD needs to be recalculated from previous RDDs.\n",
    "\n",
    "There are two types of RDD operations: transformations and actions.\n",
    "\n",
    "__Transformations__ construct a new RDD from the previous one. (mapping, filtering, etc.)\n",
    "\n",
    "__Actions__ Compute a result based on an RDD and either return it to the driver program or save it to a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- A list of transformations is set onto a RDD to make a new RDD\n",
    "- Spark constructs a Directed Acyclic Graph (DAG) based on transformations and only groups them into tasks when an action is called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- __No computations or memory is used__ until an action is called (save to HDD, report back to user, etc.)\n",
    "- Each time an action uses a RDD, it has to use all of the transformations before it\n",
    "    - Unless it's cached (saved into memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```Python\n",
    "rdd.cache()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A RDD is usually cached if the RDD is used multiple times (like in many machine learning algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To uncache a RDD:\n",
    "```Python\n",
    "rdd.unpersist()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Basic Components\n",
    "\n",
    "![alt text](images/spark-stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. __Spark Core__: The basic functionality of Spark: task scheduling, memory management, fault recovery, etc.\n",
    "2. __Spark SQL__: Spark's package for working with structured data. Provides query functionality and supports many sources of data: Hive tables, Parquet, and JSON files.\n",
    "    - Can intermix SQL queries with data manipulations supported by RDDs.\n",
    "\n",
    "3. __Spark Streaming__: Processes live streams of data.\n",
    "\n",
    "4. __MLlib__: [Machine Learning Library](https://spark.apache.org/docs/latest/mllib-guide.html) for Spark. \n",
    "\n",
    "5. __GraphX__: Library for manipulating graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Initializing a SparkConcept\n",
    "Entering though `pyspark`, `SparkR`, `spark-shell` will initialize a spark concept called `sc`. However if you wish to initialize a SparkConcept, you can do so by:\n",
    "\n",
    "```Python\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"My App\")\n",
    "sc = SparkContext(conf = conf)\n",
    "```\n",
    "And in R, this is:\n",
    "\n",
    "```R\n",
    "library(SparkR)\n",
    "sc <- sparkR.init()\n",
    "sqlContext <- sparkRSQL.init(sc)\n",
    "```\n",
    "\n",
    "- We can set the cluster URL for the master, telling Spark how to connect to the cluster.\n",
    "\n",
    "- Other properties like application configured on initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Line count\n",
    "Python Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# Set the path for spark installation\n",
    "# this is the path where you have built spark using sbt/sbt assembly\n",
    "os.environ['SPARK_HOME']=\"/usr/local/spark/current\"\n",
    "sys.path.append( os.path.join(os.environ['SPARK_HOME'], 'python') )\n",
    "sys.path.append( os.path.join(os.environ['SPARK_HOME'], 'bin') )\n",
    "sys.path.append( os.path.join(os.environ['SPARK_HOME'], 'python/lib/py4j-0.8.2.1-src.zip') )\n",
    "try:\n",
    "    from pyspark import SparkContext\n",
    "except ImportError:\n",
    "    print(\"Error importing SparkContext\")\n",
    "    sys.exit(1)\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sc = SparkContext('local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"file:/usr/local/spark/1.4.0/README.md\") # Create an RDD called lines\n",
    "lines.count() # Count the number of items in this RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Apache Spark'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.first() # First item in this RDD, i.e. first line of README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'high-level APIs in Scala, Java, and Python, and an optimized engine that'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythonLines = lines.filter(lambda line: \"Python\" in line)\n",
    "pythonLines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythonLines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stopping a SparkConcept\n",
    "\n",
    "To shutdown spark, you can call `stop()` method on your spark concept, or exit the application (i.e. closing your session of python or R)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's start up the SparkConcept again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"My App\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the above `\"local\"` is the cluster's URL, for connecting to a cluster, you would just put in the cluster's URL. The application name is helpful for identifying your application in the Spark Web UI (http://localhost:8080/ for a local machine).\n",
    "\n",
    "Additional parameters exist for configuring a Spark Job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RDD Operations\n",
    "\n",
    "- Create new RDDs, transform a RDD, & calling operations on RDDs to produce a result. \n",
    "    - Many operations on RDDs, the main ones are the methods `map` and `reduce`.\n",
    "\n",
    "These take in a function to be performed on the RDD and return a new RDD\n",
    "\n",
    "```Python\n",
    "inputFile = sc.textFile(\"filePath\")\n",
    "input_mapped = inputFile.map(mapFunction)\n",
    "output = input_mapped.reduce(reduceFunction)\n",
    "```\n",
    "\n",
    "`mapFunction` and `reduceFunction` are User Defined Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A __map__ function takes in one variable, the record or row of the RDD and returns a new record or row of the new RDD.\n",
    "\n",
    "__Example__: Map function as a lambda function:  \n",
    "```Python\n",
    "new_RDD = RDD.map(lambda x: x**2)\n",
    "```\n",
    "\n",
    "A __reduce__ function takes in two variables: two records or rows of the RDD and outputs a value.\n",
    "\n",
    "__Example__: Reduce function as a lambda function:  \n",
    "```Python\n",
    "new_RDD = RDD.reduce(lambda x,y: x+y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Note:__  \n",
    "- You can chain several methods together assigning the end result to a variable.\n",
    "\n",
    "```Python\n",
    "inputFile = sc.textFile(\"filePath\")\n",
    "output = inputFile.map(lambda x: int(x)) \\\n",
    "    .reduce(lambda x,y: x + y)\n",
    "```\n",
    "\n",
    "## Submitting Applications\n",
    "\n",
    "We can write self-contained applications using the Spark API (i.e. python scripts or compiled Java, Scala files).  \n",
    "```bash\n",
    "spark-submit --master local[4] PythonScript.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark SQL and DataFrames\n",
    "\n",
    "- For structured data\n",
    "- DataFrames are RDD's with extra information and optimized algorithms \n",
    "    - Similar R's DataFrame, except can have arrays, maps (or dictionaries), and structs as entries  \n",
    "    - DataFrames are the data structure used in Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```Python\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.json(\"examples/src/main/resources/people.json\")\n",
    "df.show() #print the contents\n",
    "## age  name\n",
    "## null Michael\n",
    "## 30   Andy\n",
    "## 19   Justin\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spark SQL (`sqlContext`) also allows SQL type of queries from DataFrames registered as tables or temporary tables: \n",
    "```Python\n",
    "df = sqlContext.sql(\"SELECT * FROM table\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Dataframes can be loaded and stored onto HDFS in multiple formats: JSON, Parquet, csv, Hive:\n",
    "```Python\n",
    "df = sqlContext.read.load(\"examples/src/main/resources/users.parquet\")\n",
    "df.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning Library (MLlib) \n",
    "\n",
    "Makes machine learning algorithms scalable\n",
    "\n",
    "- [spark.mllib](http://spark.apache.org/docs/latest/mllib-guide.html#data-types-algorithms-and-utilities) is the original API built on top of RDDs.\n",
    "- [spark.ml](http://spark.apache.org/docs/latest/ml-guide.html) high level API for DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. __Basic statistics__: Summary statistics, correlations, etc.\n",
    "\n",
    "2. __Classification & Regressions__: SVM, logistic regression, naive Bayes, Trees\n",
    "\n",
    "3. __Clustering__: k-means, \n",
    "\n",
    "4. __Dimensionality Reduction__: SVD, PCA\n",
    "\n",
    "5. __Feature Extraction & trasformation__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sudo jupyter nbconvert IntroSpark.ipynb --to slides --post serve"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
