{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Overview\n",
    "\n",
    "__Driver__: The _SparkContext_ object that coordinates the other spark processes. This driver is either run on the cluster (__cluster mode__), or on a user's laptop (__client mode__).\n",
    "\n",
    "__Cluster Manager__: Allocates resources accross applications. SparkContext can connect to SparkStandAlone, Mesos, or YARN.\n",
    "\n",
    "__Executors__: Run computations and store data for applications. Spark sends code and tasks for each executor to run.\n",
    "\n",
    "![alt text](https://spark.apache.org/docs/1.4.0/img/cluster-overview.png)\n",
    "\n",
    "- Each application gets it own set of executors.\n",
    "\n",
    "- Agnostic to the underlying cluster manager: as long as it can create executors and they communicate with each other.\n",
    "\n",
    "- Because driver schedules tasks to the cluster, it should be running close to the worker nodes: same local area network or from somewhere with a very fast connection to the cluster.\n",
    "\n",
    "\n",
    "##Cluster Manager Types\n",
    "\n",
    "__StandAlone__: Simple cluster manager included in spark.\n",
    "\n",
    "__Apache Mesos__: A general cluster manager that can run Hadoop and service apps.\n",
    "\n",
    "__Hadoop YARN__: Resource manager for Hadoop 2.\n",
    "\n",
    "##Submitting Applications\n",
    "An application can be submitted by using `spark-submit`:\n",
    "\n",
    "```bash\n",
    "$ spark-submit \\\n",
    "  --class <main-class> \\\n",
    "  --master <master-url> \\\n",
    "  --deploy-mode <deploy-mode> \\\n",
    "  --conf <key>=<value> \\\n",
    "  ... # other options\n",
    "  <application-jar> \\\n",
    "  [application-arguments]\n",
    "```\n",
    "\n",
    "If the code depends on other project, you need to package them together (except Spark and Hadoop dependencies). \n",
    "\n",
    "For Java and Scala code, you create an assembly jar or \"uber\" jar containing your code and dependencies. sbt and Maven has assembly plugins.\n",
    "\n",
    "For Python, you can use the `--py-files` argument to add a `.py`, `.zip`, or `.egg` file to be distributed with your application.\n",
    "\n",
    "For submitting code without any foreign dependencies and using the default settings:\n",
    "```\n",
    "$ spark-submit code.py\n",
    "```\n",
    "\n",
    "###Commonly Used Options\n",
    "| Option | Description | Example |\n",
    "|--------|-------------|---------|\n",
    "| `--class` | Entry point for application | ` org.apache.spark.examples.SparkPi` |\n",
    "| `--master` | Master URL for cluster | `spark://23.195.26.187:7077` |\n",
    "| `--deploy-mode` | Deployment mode for driver (`client`, `cluster`) | Default: `client` |\n",
    "| `--conf` | Arbitary Spark config property in key=value format. For values with spaces, wrap in quotes.  | `\"key=value\"` |\n",
    "\n",
    "##Monitoring Jobs\n",
    "- Each driver has a web UI, usually on port 4040: tasks, executors, storage usage. http://<driver-node>:4040\n",
    "\n",
    "Every SparkContext has a web UI, displaying useful information:\n",
    "- List of scheduler stages and tasks\n",
    "- Summary of RDD sizes and memory usage\n",
    "- Environment information\n",
    "- Information about running executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
